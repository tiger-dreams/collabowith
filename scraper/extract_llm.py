"""
CollabWith LLM Extractor
íšŒì‚¬ëª…ì„ ìž…ë ¥ë°›ì•„ ì›¹ì—ì„œ í˜‘ì—… ë„êµ¬ ì‚¬ìš© ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•˜ê³ ,
Gemini LLMìœ¼ë¡œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""

import requests
import json
import re
import time
import sys
import os
from bs4 import BeautifulSoup
import google.generativeai as genai

# Gemini API ì„¤ì •
GEMINI_API_KEY = "AIzaSyCTL0OobPOlkWvLOmqlXYtGbc5R4hZFfWA"
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel("gemini-1.5-flash")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 Chrome/120"
}

COLLAB_TOOLS = [
    "Slack", "Notion", "Zoom", "Miro", "Figma", "Jira", "Confluence",
    "Teams", "Microsoft Teams", "Google Meet", "Asana", "Monday.com",
    "Trello", "ClickUp", "Airtable", "Linear", "Loom", "Discord",
    "Webex", "Google Workspace", "GitHub", "GitLab", "Basecamp"
]


def search_articles(company: str) -> list[dict]:
    """Brave Search APIë¡œ í˜‘ì—… ë„êµ¬ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰"""
    query = f"{company} collaboration tools Slack Notion Zoom case study"
    url = "https://api.search.brave.com/res/v1/web/search"
    headers = {
        "Accept": "application/json",
        "Accept-Encoding": "gzip",
        "X-Subscription-Token": os.environ.get("BRAVE_API_KEY", "")
    }
    params = {"q": query, "count": 5, "search_lang": "en"}

    try:
        res = requests.get(url, headers=headers, params=params, timeout=10)
        data = res.json()
        results = data.get("web", {}).get("results", [])
        return [{"url": r["url"], "title": r["title"], "snippet": r.get("description", "")}
                for r in results]
    except Exception as e:
        print(f"  ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


def fetch_article_text(url: str) -> str:
    """URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(["script", "style", "nav", "footer", "header"]):
            tag.decompose()
        text = soup.get_text(separator=" ", strip=True)
        return text[:3000]  # í† í° ì ˆì•½ì„ ìœ„í•´ 3000ìžë¡œ ì œí•œ
    except Exception as e:
        return ""


def extract_with_llm(company: str, article_text: str, source_url: str) -> list[dict]:
    """Geminië¡œ í˜‘ì—… ë„êµ¬ ì •ë³´ ì¶”ì¶œ"""
    tools_list = ", ".join(COLLAB_TOOLS)
    prompt = f"""
ë‹¤ìŒ ê¸°ì‚¬ì—ì„œ '{company}' ê¸°ì—…ì´ ì‚¬ìš©í•˜ëŠ” í˜‘ì—… ë„êµ¬ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì¤˜.

í˜‘ì—… ë„êµ¬ ëª©ë¡ (ì´ ì¤‘ì—ì„œ ì°¾ì•„): {tools_list}

ê¸°ì‚¬ ë‚´ìš©:
{article_text}

ê²°ê³¼ë¥¼ ì•„ëž˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì¤˜ (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´):
[
  {{
    "tool": "ë„êµ¬ëª…",
    "use_case": "ì–´ë–¤ ìš©ë„ë¡œ ì‚¬ìš©í•˜ëŠ”ì§€ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ",
    "confidence": "high/medium/low"
  }}
]

í˜‘ì—… ë„êµ¬ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ [] ë°˜í™˜.
"""
    try:
        response = model.generate_content(prompt)
        text = response.text.strip()
        # JSON íŒŒì‹±
        match = re.search(r'\[.*?\]', text, re.DOTALL)
        if match:
            return json.loads(match.group())
    except Exception as e:
        print(f"  LLM ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    return []


def process_company(company: str) -> dict:
    """íšŒì‚¬ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print(f"\nðŸ” [{company}] ì²˜ë¦¬ ì¤‘...")
    articles = search_articles(company)
    print(f"  â†’ ê¸°ì‚¬ {len(articles)}ê±´ ë°œê²¬")

    all_tools = {}
    sources = []

    for art in articles[:3]:
        print(f"  ðŸ“„ {art['title'][:60]}...")
        text = fetch_article_text(art["url"])
        if not text:
            continue
        tools = extract_with_llm(company, text, art["url"])
        for t in tools:
            if t.get("confidence") in ["high", "medium"] and t.get("tool"):
                key = t["tool"]
                if key not in all_tools:
                    all_tools[key] = {
                        "name": key,
                        "use_case": t["use_case"],
                        "source_url": art["url"],
                        "source_type": "media_article"
                    }
        sources.append(art["url"])
        time.sleep(1)

    result = {
        "company": company,
        "domain": f"{company.lower().replace(' ', '')}.com",
        "industry": "Unknown",
        "tools": list(all_tools.values()),
        "sources": sources,
        "updated_at": time.strftime("%Y-%m-%d")
    }
    print(f"  âœ… í˜‘ì—… ë„êµ¬ {len(result['tools'])}ê°œ ì¶”ì¶œ ì™„ë£Œ")
    return result


def save_to_db(new_entry: dict, db_path: str = "../data/sample.json"):
    """DBì— ì¶”ê°€/ì—…ë°ì´íŠ¸"""
    try:
        with open(db_path, "r", encoding="utf-8") as f:
            db = json.load(f)
    except:
        db = []

    # ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸ ë˜ëŠ” ì¶”ê°€
    found = False
    for i, item in enumerate(db):
        if item["company"].lower() == new_entry["company"].lower():
            db[i] = new_entry
            found = True
            break
    if not found:
        db.append(new_entry)

    with open(db_path, "w", encoding="utf-8") as f:
        json.dump(db, f, ensure_ascii=False, indent=2)
    print(f"  ðŸ’¾ DB ì €ìž¥ ì™„ë£Œ ({db_path})")


if __name__ == "__main__":
    companies = sys.argv[1:] if len(sys.argv) > 1 else ["Samsung", "LG", "Hyundai"]
    for company in companies:
        result = process_company(company)
        if result["tools"]:
            save_to_db(result)
        time.sleep(2)
    print("\nðŸŽ‰ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")
