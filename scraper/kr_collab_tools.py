"""
Korean Collaboration Tools Scraper
í•œêµ­ í˜‘ì—… íˆ´ë“¤ì˜ ê³ ê° ì‚¬ë¡€ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import date

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36"
}

KR_VENDORS = [
    {
        "tool": "JANDI",
        "url": "https://blog.jandi.com/ko/category/user_case/",
        "base_url": "https://blog.jandi.com",
        "company_selector": "a",
        "link_pattern": "/ko/"
    },
    {
        "tool": "Dooray",
        "url": "https://helpdesk.dooray.com/share/pages/9wWo-xwiR66BO5LGshgVTg/2962315498932384699",
        "base_url": "https://dooray.com",
    },
    {
        "tool": "NAVER WORKS",
        "url": "https://naver.worksmobile.com/cases/",
        "base_url": "https://naver.worksmobile.com",
    },
    {
        "tool": "MailPlug",
        "url": "https://groupware.mailplug.com/",
        "base_url": "https://mailplug.com",
    },
]

def clean_text(text):
    return text.strip()

def scrape_jandi():
    """JANDI ë¸”ë¡œê·¸ì—ì„œ ê³ ê° ì‚¬ë¡€ ìˆ˜ì§‘"""
    print(f"\nðŸ” [JANDI] ê³ ê° ì‚¬ë¡€ ìˆ˜ì§‘ ì¤‘...")
    results = []
    try:
        res = requests.get("https://blog.jandi.com/ko/category/user_case/", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, "html.parser")

        # ë§í¬ì—ì„œ ê³ ê° ì‚¬ë¡€ ì¶”ì¶œ
        for link in soup.find_all("a", href=True):
            href = link.get("href", "")
            text = clean_text(link.get_text())

            # ê³ ê° ì‚¬ë¡€ ë§í¬ íŒ¨í„´
            if "/ko/" in href and ("customercase" in href or "user_case" in href):
                if text and len(text) > 5 and len(text) < 100:
                    # íšŒì‚¬ëª… ì¶”ì¶œ ì‹œë„ (ëŒ€ê´„í˜¸ë¡œ ê°ì‹¸ì§„ ê²½ìš°)
                    if "[" in text and "]" in text:
                        industry = text.split("[")[1].split("]")[0]
                    else:
                        industry = "Unknown"

                    full_url = href if href.startswith("http") else f"https://blog.jandi.com{href}"

                    # íšŒì‚¬ëª… ì¶”ì¶œ (ì œëª©ì—ì„œ)
                    company_name = extract_company_from_title(text)

                    if company_name:
                        results.append({
                            "company": company_name,
                            "tool": "JANDI",
                            "source_url": full_url,
                            "source_type": "vendor_case_study",
                            "use_case": f"{industry} ì—…ë¬´ í˜‘ì—… ë° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜",
                            "industry": industry,
                            "updated_at": str(date.today())
                        })

        print(f"  â†’ {len(results)}ê±´ ë°œê²¬")
    except Exception as e:
        print(f"  âŒ ì‹¤íŒ¨: {e}")
    return results

def extract_company_from_title(title):
    """ì œëª©ì—ì„œ íšŒì‚¬ëª… ì¶”ì¶œ ì‹œë„"""
    # JANDI ê³ ê° ì‚¬ë¡€ íŒ¨í„´: [[ì‚°ì—…] íšŒì‚¬ëª… ~]
    if "[" in title and "]" in title:
        after_bracket = title.split("]", 1)[1] if "]" in title else title
        # ì²« ë²ˆì§¸ ê³µë°±ê¹Œì§€ë¥¼ íšŒì‚¬ëª…ìœ¼ë¡œ ê°€ì •
        parts = after_bracket.strip().split()
        if parts:
            return parts[0]
    return None

def scrape_naver_works():
    """NAVER WORKS ë„ìž…ì‚¬ë¡€ ìˆ˜ì§‘"""
    print(f"\nðŸ” [NAVER WORKS] ë„ìž…ì‚¬ë¡€ ìˆ˜ì§‘ ì¤‘...")
    results = []
    try:
        res = requests.get("https://naver.worksmobile.com/cases/", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, "html.parser")

        # ì œëª©ì—ì„œ íšŒì‚¬ëª… ì¶”ì¶œ
        for heading in soup.find_all(["h2", "h3", "h4"]):
            text = clean_text(heading.get_text())

            # ëŒ€ë¬¸ìžë¡œ ì‹œìž‘í•˜ëŠ” íšŒì‚¬ëª… íŒ¨í„´
            if text and len(text) > 3 and len(text) < 60:
                if any(c.isupper() for c in text) or "ë„ìž…" in text or "ì‚¬ë¡€" in text:
                    results.append({
                        "company": text.split()[0] if text.split() else text,
                        "tool": "NAVER WORKS",
                        "source_url": "https://naver.worksmobile.com/cases/",
                        "source_type": "vendor_case_study",
                        "use_case": "ì—…ë¬´ìš© ë©”ì‹ ì € ë° í˜‘ì—… í”Œëž«í¼",
                        "industry": "Various",
                        "updated_at": str(date.today())
                    })

        print(f"  â†’ {len(results)}ê±´ ë°œê²¬")
    except Exception as e:
        print(f"  âŒ ì‹¤íŒ¨: {e}")
    return results

def main():
    print("ðŸ‡°ðŸ‡· í•œêµ­ í˜‘ì—… íˆ´ ê³ ê° ì‚¬ë¡€ ìˆ˜ì§‘ ì‹œìž‘\n")

    all_data = []

    # JANDI ìˆ˜ì§‘
    jandi_data = scrape_jandi()
    all_data.extend(jandi_data)
    time.sleep(2)

    # NAVER WORKS ìˆ˜ì§‘
    naver_works_data = scrape_naver_works()
    all_data.extend(naver_works_data)
    time.sleep(2)

    # ê²°ê³¼ ì €ìž¥
    output_file = "../data/kr_collab_cases.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)

    print(f"\nðŸ“Š ì´ {len(all_data)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    print(f"âœ… ì €ìž¥ â†’ {output_file}")

    # ìƒ˜í”Œ ì¶œë ¥
    print("\nðŸ“‹ ìˆ˜ì§‘ëœ ê¸°ì—… ìƒ˜í”Œ:")
    for item in all_data[:10]:
        print(f"  â€¢ {item['company']} - {item['tool']} ({item['industry']})")

if __name__ == "__main__":
    main()
