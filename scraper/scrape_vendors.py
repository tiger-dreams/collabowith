"""
CollabWith Vendor Scraper
í˜‘ì—… ë„êµ¬ ë²¤ë”ë“¤ì˜ ê³µì‹ Customer Stories í˜ì´ì§€ì—ì„œ
ê¸°ì—…ëª… + ì‚¬ìš© ì‚¬ë¡€ë¥¼ ì§ì ‘ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import date

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36"
}

VENDORS = [
    {
        "tool": "Slack",
        "url": "https://slack.com/customer-stories",
        "company_selector": "h3, h2, .customer-name, [class*='name'], [class*='title']",
    },
    {
        "tool": "Notion",
        "url": "https://www.notion.com/customers",
        "company_selector": "h3, h2, [class*='name'], [class*='company']",
    },
    {
        "tool": "Miro",
        "url": "https://miro.com/customers/",
        "company_selector": "h3, h2, [class*='name']",
    },
    {
        "tool": "Zoom",
        "url": "https://www.zoom.com/en/customer-stories/",
        "company_selector": "h3, h2, [class*='company']",
    },
    {
        "tool": "Figma",
        "url": "https://www.figma.com/customers/",
        "company_selector": "h3, h2, [class*='name']",
    },
    {
        "tool": "Atlassian",
        "url": "https://www.atlassian.com/customers",
        "company_selector": "h3, h2, [class*='name'], [class*='title']",
    },
    {
        "tool": "Asana",
        "url": "https://asana.com/customers",
        "company_selector": "h3, h2, [class*='company']",
    },
]

def clean_text(text):
    return re.sub(r'\s+', ' ', text).strip()

def scrape_vendor(vendor):
    print(f"\nğŸ” [{vendor['tool']}] {vendor['url']} í¬ë¡¤ë§ ì¤‘...")
    results = []
    try:
        res = requests.get(vendor["url"], headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, "html.parser")

        # í˜ì´ì§€ ë§í¬ì—ì„œ íšŒì‚¬ëª… ì¶”ì¶œ
        links = soup.find_all("a", href=True)
        for link in links:
            href = link.get("href", "")
            text = clean_text(link.get_text())

            # ê³ ê° ì‚¬ë¡€ ë§í¬ íŒ¨í„´ ê°ì§€
            if any(kw in href for kw in ["/customer", "/case-study", "/stories", "/customers/"]):
                if text and len(text) > 2 and len(text) < 60:
                    full_url = href if href.startswith("http") else f"https://{vendor['url'].split('/')[2]}{href}"
                    results.append({
                        "company": text,
                        "tool": vendor["tool"],
                        "source_url": full_url,
                        "source_type": "vendor_case_study",
                        "use_case": f"{vendor['tool']} í™œìš© ì‚¬ë¡€",
                        "updated_at": str(date.today())
                    })

        print(f"  â†’ {len(results)}ê±´ ë°œê²¬")
    except Exception as e:
        print(f"  âŒ ì‹¤íŒ¨: {e}")
    return results

def build_company_db(raw_data):
    """íšŒì‚¬ë³„ë¡œ ê·¸ë£¹í™”"""
    company_map = {}
    for item in raw_data:
        name = item["company"]
        if name not in company_map:
            company_map[name] = {
                "company": name,
                "domain": "",
                "industry": "Unknown",
                "tools": [],
                "updated_at": item["updated_at"]
            }
        # ì¤‘ë³µ ë„êµ¬ ë°©ì§€
        existing_tools = [t["name"] for t in company_map[name]["tools"]]
        if item["tool"] not in existing_tools:
            company_map[name]["tools"].append({
                "name": item["tool"],
                "use_case": item["use_case"],
                "source_url": item["source_url"],
                "source_type": item["source_type"]
            })
    return list(company_map.values())

if __name__ == "__main__":
    all_raw = []
    for vendor in VENDORS:
        data = scrape_vendor(vendor)
        all_raw.extend(data)
        time.sleep(2)  # Rate limit ë°©ì§€

    print(f"\nğŸ“Š ì´ {len(all_raw)}ê±´ raw ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")

    # DB êµ¬ì¡°ë¡œ ë³€í™˜
    db = build_company_db(all_raw)
    # ë„êµ¬ 2ê°œ ì´ìƒì¸ íšŒì‚¬ë§Œ í¬í•¨ (ë…¸ì´ì¦ˆ ì œê±°)
    db_filtered = [c for c in db if len(c["tools"]) >= 1]

    with open("../data/companies.json", "w", encoding="utf-8") as f:
        json.dump(db_filtered, f, ensure_ascii=False, indent=2)

    print(f"âœ… {len(db_filtered)}ê°œ ê¸°ì—… ë°ì´í„° ì €ì¥ â†’ data/companies.json")

    # ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°
    print("\nğŸ“‹ ìˆ˜ì§‘ëœ ê¸°ì—… ìƒ˜í”Œ (ìƒìœ„ 10ê°œ):")
    for c in db_filtered[:10]:
        tools = ", ".join([t["name"] for t in c["tools"]])
        print(f"  {c['company']}: {tools}")
